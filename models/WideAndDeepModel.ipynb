{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "data_path = r\"C:\\Users\\Uly\\Desktop\\Desktop\\UCL\\IRDM\\IRDM17\\src\\dict_list.p\"\n",
    "labels_path = r\"C:\\Users\\Uly\\Desktop\\Desktop\\UCL\\IRDM\\IRDM17\\src\\labels.p\"\n",
    "\n",
    "COLUMNS = ['FST_TFPD_Score','preST_TFPD_Score','postST_TFPD_Score','FST_TFPT_Score','preST_TFPT_Score',\n",
    "           'postST_TFPT_Score','FST_TFAT_Score','preST_TFAT_Score','postST_TFAT_Score','FST_WFPD_Score',\n",
    "           'preST_WFPD_Score','postST_WFPD_Score','FST_WFPT_Score','preST_WFPT_Score','postST_WFPT_Score', \n",
    "           'FST_WFAT_Score', 'preST_WFAT_Score','postST_WFAT_Score','FST_W2VPD_Score','preST_W2VPD_Score', \n",
    "           'postST_W2VPD_Score', 'prodTitle_FsT_prob', 'prodTitle_length_feat','prodTitle_postST_prob', \n",
    "           'prodTitle_preST_prob', 'prodDesc_FsT_prob', 'prodDesc_length_feat','prodDesc_postST_prob', \n",
    "           'prodDesc_preST_prob', 'prodAttr_FsT_prob', 'prodAttr_length_feat', 'prodAttr_postST_prob', \n",
    "           'prodAttr_preST_prob']\n",
    "dnn_hidden_layers_param = [1024, 512, 256]\n",
    "dnn_dropout_param = 0.2\n",
    "learning_rate_param = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir=None):\n",
    "    #Columns\n",
    "    FST_TFPD_Score= tf.contrib.layers.real_valued_column('FST_TFPD_Score')\n",
    "    preST_TFPD_Score= tf.contrib.layers.real_valued_column('preST_TFPD_Score')\n",
    "    postST_TFPD_Score= tf.contrib.layers.real_valued_column('postST_TFPD_Score')\n",
    "    FST_TFPT_Score= tf.contrib.layers.real_valued_column('FST_TFPT_Score')\n",
    "    preST_TFPT_Score= tf.contrib.layers.real_valued_column('preST_TFPT_Score')\n",
    "    postST_TFPT_Score= tf.contrib.layers.real_valued_column('postST_TFPT_Score')\n",
    "    FST_TFAT_Score= tf.contrib.layers.real_valued_column('FST_TFAT_Score')\n",
    "    preST_TFAT_Score= tf.contrib.layers.real_valued_column('preST_TFAT_Score')\n",
    "    postST_TFAT_Score= tf.contrib.layers.real_valued_column('postST_TFAT_Score')\n",
    "    FST_WFPD_Score= tf.contrib.layers.real_valued_column('FST_WFPD_Score')\n",
    "    preST_WFPD_Score= tf.contrib.layers.real_valued_column('preST_WFPD_Score')\n",
    "    postST_WFPD_Score= tf.contrib.layers.real_valued_column('postST_WFPD_Score')\n",
    "    FST_WFPT_Score= tf.contrib.layers.real_valued_column('FST_WFPT_Score')  \n",
    "    preST_WFPT_Score= tf.contrib.layers.real_valued_column('preST_WFPT_Score')  \n",
    "    postST_WFPT_Score= tf.contrib.layers.real_valued_column('postST_WFPT_Score')  \n",
    "    FST_WFAT_Score= tf.contrib.layers.real_valued_column('FST_WFAT_Score')  \n",
    "    preST_WFAT_Score= tf.contrib.layers.real_valued_column('preST_WFAT_Score')  \n",
    "    postST_WFAT_Score= tf.contrib.layers.real_valued_column('postST_WFAT_Score')  \n",
    "    FST_W2VPD_Score= tf.contrib.layers.real_valued_column('FST_W2VPD_Score')  \n",
    "    preST_W2VPD_Score= tf.contrib.layers.real_valued_column('preST_W2VPD_Score')  \n",
    "    postST_W2VPD_Score= tf.contrib.layers.real_valued_column('postST_W2VPD_Score')  \n",
    "    prodTitle_FsT_prob= tf.contrib.layers.real_valued_column('prodTitle_FsT_prob')  \n",
    "    prodTitle_length_feat= tf.contrib.layers.real_valued_column('prodTitle_length_feat')\n",
    "    prodTitle_postST_prob= tf.contrib.layers.real_valued_column('prodTitle_postST_prob')  \n",
    "    prodTitle_preST_prob= tf.contrib.layers.real_valued_column('prodTitle_preST_prob')  \n",
    "    prodDesc_FsT_prob= tf.contrib.layers.real_valued_column('prodDesc_FsT_prob')  \n",
    "    prodDesc_length_feat= tf.contrib.layers.real_valued_column('prodDesc_length_feat')  \n",
    "    prodDesc_postST_prob= tf.contrib.layers.real_valued_column('prodDesc_postST_prob')  \n",
    "    prodDesc_preST_prob= tf.contrib.layers.real_valued_column('prodDesc_preST_prob')  \n",
    "    prodAttr_FsT_prob= tf.contrib.layers.real_valued_column('prodAttr_FsT_prob')  \n",
    "    prodAttr_length_feat= tf.contrib.layers.real_valued_column('prodAttr_length_feat')  \n",
    "    prodAttr_postST_prob= tf.contrib.layers.real_valued_column('prodAttr_postST_prob')  \n",
    "    prodAttr_preST_prob= tf.contrib.layers.real_valued_column('prodAttr_preST_prob')\n",
    "    #common_word_count= tf.contrib.layers.real_valued_column('common_word_count')\n",
    "    \n",
    "    wide_columns = [FST_TFPD_Score,preST_TFPD_Score,postST_TFPD_Score,FST_TFPT_Score,preST_TFPT_Score,\n",
    "           postST_TFPT_Score,FST_TFAT_Score,preST_TFAT_Score,postST_TFAT_Score,FST_WFPD_Score,\n",
    "           preST_WFPD_Score,postST_WFPD_Score,FST_WFPT_Score,preST_WFPT_Score,postST_WFPT_Score, \n",
    "           FST_WFAT_Score, preST_WFAT_Score,postST_WFAT_Score,FST_W2VPD_Score,preST_W2VPD_Score, \n",
    "           postST_W2VPD_Score, prodTitle_FsT_prob, prodTitle_length_feat,prodTitle_postST_prob, \n",
    "           prodTitle_preST_prob, prodDesc_FsT_prob, prodDesc_length_feat,prodDesc_postST_prob, \n",
    "           prodDesc_preST_prob, prodAttr_FsT_prob, prodAttr_length_feat, prodAttr_postST_prob, \n",
    "           prodAttr_preST_prob]\n",
    "\n",
    "    deep_columns = [FST_TFPD_Score,preST_TFPD_Score,postST_TFPD_Score,FST_TFPT_Score,preST_TFPT_Score,\n",
    "           postST_TFPT_Score,FST_TFAT_Score,preST_TFAT_Score,postST_TFAT_Score,FST_WFPD_Score,\n",
    "           preST_WFPD_Score,postST_WFPD_Score,FST_WFPT_Score,preST_WFPT_Score,postST_WFPT_Score, \n",
    "           FST_WFAT_Score, preST_WFAT_Score,postST_WFAT_Score,FST_W2VPD_Score,preST_W2VPD_Score, \n",
    "           postST_W2VPD_Score, prodTitle_FsT_prob, prodTitle_length_feat,prodTitle_postST_prob, \n",
    "           prodTitle_preST_prob, prodDesc_FsT_prob, prodDesc_length_feat,prodDesc_postST_prob, \n",
    "           prodDesc_preST_prob, prodAttr_FsT_prob, prodAttr_length_feat, prodAttr_postST_prob, \n",
    "           prodAttr_preST_prob]\n",
    "    \n",
    "    estimator = tf.contrib.learn.DNNLinearCombinedRegressor(\n",
    "        # wide settings\n",
    "        linear_feature_columns=wide_columns,\n",
    "        linear_optimizer=tf.train.FtrlOptimizer(learning_rate=learning_rate_param,\n",
    "                                                l1_regularization_strength=0.001,\n",
    "                                                l2_regularization_strength=0.001),\n",
    "        # deep settings\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        #dnn_hidden_units=[1000, 500, 100],\n",
    "        dnn_hidden_units=dnn_hidden_layers_param,\n",
    "        dnn_optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.1,\n",
    "                                                        l1_regularization_strength=0.001,\n",
    "                                                        l2_regularization_strength=0.001),\n",
    "        config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1800),\n",
    "        dnn_dropout=dnn_dropout_param\n",
    "    )\n",
    "\n",
    "    \n",
    "    return estimator\n",
    "\n",
    "def input_fn(data,labels, first=None, last=None):\n",
    "    feature_cols = {}\n",
    "    for k in COLUMNS:\n",
    "        feature_cols[k] = tf.constant(data[k][first:last])\n",
    "    \n",
    "    labels = tf.constant(labels[first:last])\n",
    "    return feature_cols, labels\n",
    "\n",
    "def train(training_data,training_labels,first=None,last=None,model_dir=None, train_steps=200):\n",
    "    features,labels = input_fn(training_data,training_labels)\n",
    "    m = build_estimator()\n",
    "    m.fit(input_fn=lambda: input_fn(training_data,training_labels,first,last), steps=train_steps)\n",
    "    #results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n",
    "\n",
    "    return m\n",
    "\n",
    "def load_data(path):\n",
    "    return pickle.load(open(path,\"rb\"))\n",
    "\n",
    "#def evaluation(model, validation_data, validation_labels,model_type):\n",
    "#    return model.predict(input_fn=lambda: input_fn(validation_data,validation_labels,model_type), as_iterable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Loading data')\n",
    "training_data = load_data(data_path)\n",
    "training_labels = load_data(labels_path)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************\n",
      "[1024, 512, 256]\n",
      "0.2\n",
      "0.1\n",
      "--------------- Starting round:1/3---------------\n",
      "Spliting data randomly\n",
      "Training model\n",
      "Model trained in 1772.7275891304016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG:0.861169602404\n",
      "RMSE:0.820035567295\n",
      "--------------- Starting round:2/3---------------\n",
      "Spliting data randomly\n",
      "Training model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-51c1aa1744c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[1;31m# TODO train your model here with features_train as an input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelevance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[1;31m#model = RandomForestRegressor(n_estimators = 10, max_depth=5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[1;31m#model.fit(y=Y, X= features_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-12d3fb8a45e7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(training_data, training_labels, first, last, model_dir, train_steps)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[1;31m#results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             func.__module__, arg_name, date, instructions)\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    193\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    353\u001b[0m                              \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                              \u001b[0mmonitors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmonitors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                              max_steps=max_steps)\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\u001b[0m\n\u001b[1;32m    731\u001b[0m           \u001b[0mfail_on_nan_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfail_on_nan_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           max_steps=max_steps)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extract_metric_update_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\graph_actions.py\u001b[0m in \u001b[0;36m_monitored_train\u001b[0;34m(graph, output_dir, train_op, loss_op, global_step_tensor, init_op, init_feed_dict, init_fn, log_every_steps, supervisor_is_chief, supervisor_master, supervisor_save_model_secs, supervisor_save_model_steps, keep_checkpoint_max, supervisor_save_summaries_secs, supervisor_save_summaries_steps, feed_fn, steps, fail_on_nan_loss, hooks, max_steps)\u001b[0m\n\u001b[1;32m    299\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msuper_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         _, loss = super_sess.run([train_op, loss_op], feed_fn() if feed_fn else\n\u001b[0;32m--> 301\u001b[0;31m                                  None)\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0msummary_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSummaryWriterCache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    471\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    626\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    629\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAbortedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    727\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Uly\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# input: an ordered vector of relevance, output: Discountegd Cumulative Gain\n",
    "def DCG(vec):\n",
    "    sc = 0\n",
    "    for i in range(1,len(vec)):\n",
    "        sc += ((2**vec[i-1])-1)/math.log(i+1, 2)\n",
    "    return sc\n",
    " \n",
    "################################## Loading dat and matrix###########################################\n",
    "#with open(\"the_matrix_0feat.p\", 'rb') as file:\n",
    "#    features_mat = pickle.load(file)\n",
    "\n",
    "df_all = pd.read_csv(r\"C:\\Users\\Uly\\Desktop\\Desktop\\UCL\\IRDM\\IRDM17\\src\\df_all.csv\", encoding=\"ISO-8859-1\")\n",
    "################################## Main code #######################################################\n",
    " \n",
    "unique_search_term = list(set(df_all.search_term))\n",
    "# K fold cross validation\n",
    "K = 3\n",
    "scores_in_cross_DCG = np.empty(K)\n",
    "scores_in_cross_RMSE = np.empty(K)\n",
    "#percentage of the training set set asside for testing in k cross validation\n",
    "test_percentage = 0.4\n",
    "print(\"*******************\")\n",
    "print(dnn_hidden_layers_param)\n",
    "print(dnn_dropout_param)\n",
    "print(learning_rate_param)\n",
    "for k in range(K):\n",
    "    print(\"--------------- Starting round:\" + str(k+1) + \"/\" + str(K) + \"---------------\")\n",
    "    print(\"Spliting data randomly\")\n",
    "    # we select some random query for test set\n",
    "    test_queries = random.sample(unique_search_term,round(test_percentage*len(unique_search_term)))\n",
    "    train_queries = list(set(unique_search_term)-set(test_queries))\n",
    "    # we select the sub data frame with only the train and test_queries\n",
    "    df_train = df_all.loc[df_all['search_term'].isin(train_queries)]\n",
    "    df_test = df_all.loc[df_all['search_term'].isin(test_queries)]\n",
    " \n",
    "    # we then get the indexes for the feature matrix split\n",
    "    ind_train = df_all[df_all['search_term'].isin(train_queries)].index.tolist()\n",
    "    ind_test = df_all[df_all['search_term'].isin(test_queries)].index.tolist()\n",
    "    \n",
    "    features_train = {}\n",
    "    features_test = {}\n",
    "    \n",
    "    for key in COLUMNS:\n",
    "        features_train[key] = [training_data[key][i] for i in ind_train]\n",
    "        features_test[key] = [training_data[key][i] for i in ind_test]\n",
    " \n",
    "    ####################### train the model on df_train #####################################\n",
    "    print(\"Training model\")\n",
    "    start = time.time()\n",
    "    # TODO train your model here with features_train as an input\n",
    "    Y = df_train.relevance\n",
    "    model = train(features_train,Y.tolist())\n",
    "    #model = RandomForestRegressor(n_estimators = 10, max_depth=5)\n",
    "    #model.fit(y=Y, X= features_train)\n",
    "    end = time.time()\n",
    "    print(\"Model trained in\",end - start)\n",
    "    ####################### testing #########################################################\n",
    "    # TODO apply your model to the df_test and put the predicted relevance in the \"est_relevance\" column\n",
    "    estimated_relevance = model.predict(input_fn=lambda: input_fn(features_test,Y.tolist()), \n",
    "                                        as_iterable=False)\n",
    "    #estimated_relevance  = np.random.randint(3, size=(len(df_test.search_term)))\n",
    "    df_test[\"est_relevance\"] = np.clip(estimated_relevance,1,3) # random solution, np.random.randint(3, size=(len(df_test.search_term)))\n",
    "    # Computing the final score\n",
    "    # First we create a list of documents organized by our model\n",
    "    final_k_score_DCG = 0\n",
    "    c = 0\n",
    "    #computing the dcg score\n",
    "    for query in test_queries:\n",
    "        df_temp = df_test.loc[df_test.search_term == query]\n",
    "        df_temp = df_temp.sort(['est_relevance'],ascending = False)\n",
    "        if len(df_temp.relevance)>1:\n",
    "            c += 1\n",
    "            # applying the normalized DCG by computing it and divided it by the perfect DCG score\n",
    "            final_k_score_DCG += DCG(np.array(df_temp.relevance))/DCG(-np.sort(-np.array(df_temp.relevance)))\n",
    "            #print(DCG(np.array(df_temp.relevance))/DCG(-np.sort(-np.array(df_temp.relevance))))\n",
    "    # divding the usm of normaized DCG score by the number of queries with more than 1 document\n",
    "    final_k_score_DCG = final_k_score_DCG/c\n",
    "    # computing the root means square error\n",
    "    y = np.array(df_test.relevance)\n",
    "    y_hat = np.array(df_test.est_relevance)\n",
    "    final_k_score_RMSE = np.power(np.sum(np.power(y - y_hat, 2)) / len(y), 0.5)\n",
    " \n",
    "    print('DCG:' + str(final_k_score_DCG))\n",
    "    print('RMSE:' + str(final_k_score_RMSE))\n",
    " \n",
    "    #saving final score\n",
    "    scores_in_cross_DCG[k] = final_k_score_DCG\n",
    "    scores_in_cross_RMSE[k] = final_k_score_RMSE\n",
    "\n",
    "print(\"Cross validation procedure completed\")\n",
    "print(\"DCG score vector \" + str(scores_in_cross_DCG))\n",
    "print(\"DCG mean score \" + str(np.mean(scores_in_cross_DCG)))\n",
    "print(\"RMSE score vector \" + str(scores_in_cross_RMSE))\n",
    "print(\"RMSE mean score \" + str(np.mean(scores_in_cross_RMSE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
