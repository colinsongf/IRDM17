{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "data_path = r\"..\\data\\dict_list.p\"\n",
    "labels_path = r\"..\\data\\labels.p\"\n",
    "\n",
    "COLUMNS = ['FST_TFPD_Score','preST_TFPD_Score','postST_TFPD_Score','FST_TFPT_Score','preST_TFPT_Score',\n",
    "           'postST_TFPT_Score','FST_TFAT_Score','preST_TFAT_Score','postST_TFAT_Score','FST_WFPD_Score',\n",
    "           'preST_WFPD_Score','postST_WFPD_Score','FST_WFPT_Score','preST_WFPT_Score','postST_WFPT_Score', \n",
    "           'FST_WFAT_Score', 'preST_WFAT_Score','postST_WFAT_Score','FST_W2VPD_Score','preST_W2VPD_Score', \n",
    "           'postST_W2VPD_Score', 'prodTitle_FsT_prob', 'prodTitle_length_feat','prodTitle_postST_prob', \n",
    "           'prodTitle_preST_prob', 'prodDesc_FsT_prob', 'prodDesc_length_feat','prodDesc_postST_prob', \n",
    "           'prodDesc_preST_prob', 'prodAttr_FsT_prob', 'prodAttr_length_feat', 'prodAttr_postST_prob', \n",
    "           'prodAttr_preST_prob']\n",
    "\n",
    "dnn_hidden_layers_param = [10,5]\n",
    "learning_rate_param = 0.1\n",
    "steps = 10000\n",
    "adam_op = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "adagrad_op = tf.train.ProximalAdagradOptimizer(learning_rate=learning_rate_param,l1_regularization_strength=0.001,\n",
    "                                               l2_regularization_strength=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir=None):\n",
    "    FST_TFPD_Score= tf.contrib.layers.real_valued_column('FST_TFPD_Score')\n",
    "    preST_TFPD_Score= tf.contrib.layers.real_valued_column('preST_TFPD_Score')\n",
    "    postST_TFPD_Score= tf.contrib.layers.real_valued_column('postST_TFPD_Score')\n",
    "    FST_TFPT_Score= tf.contrib.layers.real_valued_column('FST_TFPT_Score')\n",
    "    preST_TFPT_Score= tf.contrib.layers.real_valued_column('preST_TFPT_Score')\n",
    "    postST_TFPT_Score= tf.contrib.layers.real_valued_column('postST_TFPT_Score')\n",
    "    FST_TFAT_Score= tf.contrib.layers.real_valued_column('FST_TFAT_Score')\n",
    "    preST_TFAT_Score= tf.contrib.layers.real_valued_column('preST_TFAT_Score')\n",
    "    postST_TFAT_Score= tf.contrib.layers.real_valued_column('postST_TFAT_Score')\n",
    "    FST_WFPD_Score= tf.contrib.layers.real_valued_column('FST_WFPD_Score')\n",
    "    preST_WFPD_Score= tf.contrib.layers.real_valued_column('preST_WFPD_Score')\n",
    "    postST_WFPD_Score= tf.contrib.layers.real_valued_column('postST_WFPD_Score')\n",
    "    FST_WFPT_Score= tf.contrib.layers.real_valued_column('FST_WFPT_Score')  \n",
    "    preST_WFPT_Score= tf.contrib.layers.real_valued_column('preST_WFPT_Score')  \n",
    "    postST_WFPT_Score= tf.contrib.layers.real_valued_column('postST_WFPT_Score')  \n",
    "    FST_WFAT_Score= tf.contrib.layers.real_valued_column('FST_WFAT_Score')  \n",
    "    preST_WFAT_Score= tf.contrib.layers.real_valued_column('preST_WFAT_Score')  \n",
    "    postST_WFAT_Score= tf.contrib.layers.real_valued_column('postST_WFAT_Score')  \n",
    "    FST_W2VPD_Score= tf.contrib.layers.real_valued_column('FST_W2VPD_Score')  \n",
    "    preST_W2VPD_Score= tf.contrib.layers.real_valued_column('preST_W2VPD_Score')  \n",
    "    postST_W2VPD_Score= tf.contrib.layers.real_valued_column('postST_W2VPD_Score')  \n",
    "    prodTitle_FsT_prob= tf.contrib.layers.real_valued_column('prodTitle_FsT_prob')  \n",
    "    prodTitle_length_feat= tf.contrib.layers.real_valued_column('prodTitle_length_feat')\n",
    "    prodTitle_postST_prob= tf.contrib.layers.real_valued_column('prodTitle_postST_prob')  \n",
    "    prodTitle_preST_prob= tf.contrib.layers.real_valued_column('prodTitle_preST_prob')  \n",
    "    prodDesc_FsT_prob= tf.contrib.layers.real_valued_column('prodDesc_FsT_prob')  \n",
    "    prodDesc_length_feat= tf.contrib.layers.real_valued_column('prodDesc_length_feat')  \n",
    "    prodDesc_postST_prob= tf.contrib.layers.real_valued_column('prodDesc_postST_prob')  \n",
    "    prodDesc_preST_prob= tf.contrib.layers.real_valued_column('prodDesc_preST_prob')  \n",
    "    prodAttr_FsT_prob= tf.contrib.layers.real_valued_column('prodAttr_FsT_prob')  \n",
    "    prodAttr_length_feat= tf.contrib.layers.real_valued_column('prodAttr_length_feat')  \n",
    "    prodAttr_postST_prob= tf.contrib.layers.real_valued_column('prodAttr_postST_prob')  \n",
    "    prodAttr_preST_prob= tf.contrib.layers.real_valued_column('prodAttr_preST_prob')\n",
    "    \n",
    "    wide_columns = [FST_TFPD_Score,preST_TFPD_Score,postST_TFPD_Score,FST_TFPT_Score,preST_TFPT_Score,\n",
    "           postST_TFPT_Score,FST_TFAT_Score,preST_TFAT_Score,postST_TFAT_Score,FST_WFPD_Score,\n",
    "           preST_WFPD_Score,postST_WFPD_Score,FST_WFPT_Score,preST_WFPT_Score,postST_WFPT_Score, \n",
    "           FST_WFAT_Score, preST_WFAT_Score,postST_WFAT_Score,FST_W2VPD_Score,preST_W2VPD_Score, \n",
    "           postST_W2VPD_Score, prodTitle_FsT_prob, prodTitle_length_feat,prodTitle_postST_prob, \n",
    "           prodTitle_preST_prob, prodDesc_FsT_prob, prodDesc_length_feat,prodDesc_postST_prob, \n",
    "           prodDesc_preST_prob, prodAttr_FsT_prob, prodAttr_length_feat, prodAttr_postST_prob, \n",
    "           prodAttr_preST_prob]\n",
    "\n",
    "    deep_columns = [FST_TFPD_Score,preST_TFPD_Score,postST_TFPD_Score,FST_TFPT_Score,preST_TFPT_Score,\n",
    "           postST_TFPT_Score,FST_TFAT_Score,preST_TFAT_Score,postST_TFAT_Score,FST_WFPD_Score,\n",
    "           preST_WFPD_Score,postST_WFPD_Score,FST_WFPT_Score,preST_WFPT_Score,postST_WFPT_Score, \n",
    "           FST_WFAT_Score, preST_WFAT_Score,postST_WFAT_Score,FST_W2VPD_Score,preST_W2VPD_Score, \n",
    "           postST_W2VPD_Score, prodTitle_FsT_prob, prodTitle_length_feat,prodTitle_postST_prob, \n",
    "           prodTitle_preST_prob, prodDesc_FsT_prob, prodDesc_length_feat,prodDesc_postST_prob, \n",
    "           prodDesc_preST_prob, prodAttr_FsT_prob, prodAttr_length_feat, prodAttr_postST_prob, \n",
    "           prodAttr_preST_prob]\n",
    "\n",
    "    estimator = tf.contrib.learn.DNNLinearCombinedRegressor(\n",
    "        # wide settings\n",
    "        linear_feature_columns=wide_columns,\n",
    "        linear_optimizer=tf.train.FtrlOptimizer(learning_rate=learning_rate_param,\n",
    "                                                l1_regularization_strength=0.001,\n",
    "                                                l2_regularization_strength=0.001),\n",
    "        # deep settings\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=dnn_hidden_layers_param,\n",
    "        dnn_optimizer=adagrad_op,\n",
    "        config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1800)\n",
    "    )\n",
    "\n",
    "    return estimator\n",
    "\n",
    "def input_fn(data,labels):\n",
    "    feature_cols = {}\n",
    "    for k in COLUMNS:\n",
    "        feature_cols[k] = tf.constant(data[k])\n",
    "    \n",
    "    labels = tf.constant(labels)\n",
    "    return feature_cols, labels\n",
    "\n",
    "def train(training_data,training_labels,model_dir=None, train_steps=steps):\n",
    "    m = build_estimator()\n",
    "    m.fit(input_fn=lambda: input_fn(training_data,training_labels), steps=train_steps)\n",
    "\n",
    "    return m\n",
    "\n",
    "def load_data(path):\n",
    "    return pickle.load(open(path,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Loading data')\n",
    "training_data = load_data(data_path)\n",
    "training_labels = load_data(labels_path)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input: an ordered vector of relevance, output: Discountegd Cumulative Gain\n",
    "def DCG(vec):\n",
    "    sc = 0\n",
    "    for i in range(1,len(vec)):\n",
    "        sc += ((2**vec[i-1])-1)/math.log(i+1, 2)\n",
    "    return sc\n",
    " \n",
    "################################## Loading dat and matrix###########################################\n",
    "df_all = pd.read_csv(r\"..\\data\\df_all.csv\", encoding=\"ISO-8859-1\")\n",
    "################################## Main code #######################################################\n",
    " \n",
    "unique_search_term = list(set(df_all.search_term))\n",
    "# K fold cross validation\n",
    "K = 3\n",
    "scores_in_cross_DCG = np.empty(K)\n",
    "scores_in_cross_RMSE = np.empty(K)\n",
    "#percentage of the training set set asside for testing in k cross validation\n",
    "test_percentage = 0.4\n",
    "\n",
    "for k in range(K):\n",
    "    print(\"--------------- Starting round:\" + str(k+1) + \"/\" + str(K) + \"---------------\")\n",
    "    print(\"Spliting data randomly\")\n",
    "    # we select some random query for test set\n",
    "    test_queries = random.sample(unique_search_term,round(test_percentage*len(unique_search_term)))\n",
    "    train_queries = list(set(unique_search_term)-set(test_queries))\n",
    "    # we select the sub data frame with only the train and test_queries\n",
    "    df_train = df_all.loc[df_all['search_term'].isin(train_queries)]\n",
    "    df_test = df_all.loc[df_all['search_term'].isin(test_queries)]\n",
    "    \n",
    "    # we then get the indexes for the feature matrix split\n",
    "    ind_train = df_all[df_all['search_term'].isin(train_queries)].index.tolist()\n",
    "    ind_test = df_all[df_all['search_term'].isin(test_queries)].index.tolist()\n",
    "    \n",
    "    features_train = {}\n",
    "    features_test = {}\n",
    "    labels_train = [training_labels[i] for i in ind_train]\n",
    "    labels_test = [training_labels[i] for i in ind_test]\n",
    "    \n",
    "    for key in COLUMNS:\n",
    "        features_train[key] = [training_data[key][i] for i in ind_train]\n",
    "        features_test[key] = [training_data[key][i] for i in ind_test]\n",
    "    ####################### train the model on df_train #####################################\n",
    "    print(\"Training model\")\n",
    "    start = time.time()\n",
    "    # TODO train your model here with features_train as an input\n",
    "    #Y = df_train.relevance\n",
    "    \n",
    "    model = train(features_train,labels_train)\n",
    "    #model = RandomForestRegressor(n_estimators = 10, max_depth=5)\n",
    "    #model.fit(y=Y, X= features_train)\n",
    "    end = time.time()\n",
    "    print(\"Model trained in\",end - start)\n",
    "    ####################### testing #########################################################\n",
    "    # TODO apply your model to the df_test and put the predicted relevance in the \"est_relevance\" column\n",
    "    estimated_relevance = model.predict(input_fn=lambda: input_fn(features_test,labels_train), \n",
    "                                        as_iterable=False)\n",
    "    print(estimated_relevance)\n",
    "    #estimated_relevance  = np.random.randint(3, size=(len(df_test.search_term)))\n",
    "    df_test[\"est_relevance\"] = np.clip(estimated_relevance,1,3) # random solution, np.random.randint(3, size=(len(df_test.search_term)))\n",
    "    # Computing the final score\n",
    "    # First we create a list of documents organized by our model\n",
    "    final_k_score_DCG = 0\n",
    "    c = 0\n",
    "    #computing the dcg score\n",
    "    for query in test_queries:\n",
    "        df_temp = df_test.loc[df_test.search_term == query]\n",
    "        df_temp = df_temp.sort(['est_relevance'],ascending = False)\n",
    "        if len(df_temp.relevance)>1:\n",
    "            c += 1\n",
    "            # applying the normalized DCG by computing it and divided it by the perfect DCG score\n",
    "            final_k_score_DCG += DCG(np.array(df_temp.relevance))/DCG(-np.sort(-np.array(df_temp.relevance)))\n",
    "            #print(DCG(np.array(df_temp.relevance))/DCG(-np.sort(-np.array(df_temp.relevance))))\n",
    "    # divding the usm of normaized DCG score by the number of queries with more than 1 document\n",
    "    final_k_score_DCG = final_k_score_DCG/c\n",
    "    # computing the root means square error\n",
    "    #y = np.array(df_test.relevance)\n",
    "    y = np.array(labels_test)\n",
    "    y_hat = np.array(df_test.est_relevance)\n",
    "    final_k_score_RMSE = np.power(np.sum(np.power(y - y_hat, 2)) / len(y), 0.5)\n",
    " \n",
    "    print('DCG:' + str(final_k_score_DCG))\n",
    "    print('RMSE:' + str(final_k_score_RMSE))\n",
    " \n",
    "    #saving final score\n",
    "    scores_in_cross_DCG[k] = final_k_score_DCG\n",
    "    scores_in_cross_RMSE[k] = final_k_score_RMSE\n",
    "\n",
    "print(\"Cross validation procedure completed\")\n",
    "print(\"DCG score vector \" + str(scores_in_cross_DCG))\n",
    "print(\"DCG mean score \" + str(np.mean(scores_in_cross_DCG)))\n",
    "print(\"RMSE score vector \" + str(scores_in_cross_RMSE))\n",
    "print(\"RMSE mean score \" + str(np.mean(scores_in_cross_RMSE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
